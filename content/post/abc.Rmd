---
title: "ABC in diagrams"
author: "Alison Parton"
date: 2018-04-11
output: html_document
tags: ["soay-abc","ABC","Bayesian"]
bibliography: abc_bib.bib
summary: "This is an introduction to simple, rejection ABC and sequential ABC methods."
header-includes:
  - \usepackage{pdfpages}
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

This is a quick introduction to simple, rejection approximate Bayesian compution and more useful sequential methods (because I needed to make the diagrams for a poster).

## Simple Rejection ABC

Many of the tools for inference about a parameter set $\Theta$ given observed data $x$ require the ability to evaluate the likelihood $f(\Theta | x)$. When this likelihood is analytically or computationally intractable, approximate Bayesian computation (ABC) allows for likelihood-free inference when the model can be simulated from, and is a popular tool in areas of genetics, climate study and ecology.

The simplest implementation of ABC is a rejection sampler [@Pritchard1999], with schematic diagram shown:

![Fig 1: Simple rejection ABC](/post/Simple_ABC.png)

- A set of $N$ "particles" (i.e. parameter sets) are proposed from the prior distribution and each is used to generate a simulated set of data under the model ($x_i \sim f(x_i~|\Theta_i)$). 
- Each of these simulated datasets are compared with the observed data, $x$, using by taking the distance (usually the Euclidean) between a set of summary statistics evaluated at $x$ and $x_i$. In the above schematic, the 'summary' of the data is the colour. 
- Particles are accepted by either setting a tolerance threshold for the distance, or more commonly, by accepting the $\alpha \%$ of particles with the smallest distance (i.e. the set of particles that led to the 'best'/most similar simulations). This sample of parameters is an approximation to the target posterior distribution. 

## Sequential ABC

The simple ABC method often leads to low acceptance rates of parameters, or the acceptance of parameters with large distances from the observed data resulting in a poor approximation to the posterior. This is especially so when the prior distribution is very different from the true posterior. Rather than completing a single step from prior to posterior that would require a computationally inefficient number of proposed particles and corresponding simulations, sequential methods have been proposed that iterate over a sequence of distributions from the prior that get ever 'closer' to the posterior. The idea here is to have a higehr acceptance at each step than would be required by the simple rejection method, with each step focusing further on the regions of parameter space with high likelihood, with the hope of reducing the computational burden by requiring fewer iterations overall than simple rejection.

A number of sequential ABC methods have been proposed based on the same idea that differ slightly in implementation. @Sisson2009 and @Toni2009 require a set of decreasing tolerance levels for the distance function at each step to be specified, which is unideal as the appropriate levels are difficult to ascertain. @Drovandi2011 and @DelMoral2012 do not require this, and instead estimate an appropriate tolerance level at each step, however these methods can lead to the duplication of particles. 

This post will focus on the @Lenormand2013 approach. This approach has an automated tolerance level that is based on accepting the best set of particles of the same size each step but also allows for particles from previous steps to remain throughout future steps if they are still within the set of "best" particles. This is an attractive feature, computationally, as you retain particles with high likelihood. Additionally, the other methods described require repeated simulations at each step until a certain number of particles has been accepted. In @Lenormand2013 the nubmer of simulations at each step is constant, and so the computational cost of another step is always known.

The so-called adaptive population Monte Carlo (APMC) ABC sampler is shown in the following schematic:

![Fig 2: Sequential ABC](/post/APMC-ABC.png)

- An initial step the same as simple rejection ABC is carried out, retaining the $\alpha N$ particles with smallest distance to the observed data. 
- Each iterative step then consists of taking the set of retained particles and taking a weighted resample of $(1-\alpha)N$ particles. This sample is perturbed (usually by a normal random walk with variance determined by the variation in the current retained particle set) to give the set of proposed particles.
- As in simple ABC, the proposed particles are used to simulate data from the model.
- The simulated data from the retained particles and the new proposed particles are combined, compared with the observed data, and the $\alpha N$ particles from this set of $N$ with the smallest distance are retained. This forms the new "best" set of particles and corresponds to a tolerance level of the $\alpha$ percentile of the distances (therefore guaranteed to decrease at each step).
- The set of weights for the retained particles that were proposed this step are calculated. This weight is based on the weight from an importance sample, i.e. how likely it was to propose that particular particle.
- These steps are repeated until a threshold is met regarding the number of new particles being accepted over the ones retained from the previous step - i.e. when the posterior is not changing because new particles aren't being accepted, there is little to be gained from further steps.

## References