---
title: "Logistic regression: R vs Julia"
author: Alison Parton
output: html_document
date: 2018-02-22
tags: ["R","julia","glm"]
---



<p>This is a quick comparison of simple logistic regression fitting in R and julia. To implement this, you will need julia and the <code>JuliaCall</code> package in R. The following allows you to define a <code>julia chunk</code> in R markdown.</p>
<pre class="r"><code>knitr::knit_engines$set(julia = JuliaCall::eng_juliacall)</code></pre>
<div id="in-r" class="section level2">
<h2>In R</h2>
<p>Standard logistic regression is implemented easily in R with the <code>glm</code> function. In this example we’ll use the following data, that gives a continuous variable <code>x</code> and the numbers of successes, <code>succ</code>, and failures <code>fail</code>. The total population size for each observation of <code>x</code> is then successes plus failures.</p>
<pre class="r"><code>obs &lt;- data.frame(x = c(1.1, 3.3, 5.3, 2.1, 7.3, 3.6, 5.9, 4.6, 3.5, 4.3),
                  succ = c(23, 24, 17, 26, 15, 22, 20, 19, 19, 21),
                  fail = c(39, 35, 32, 28, 25, 31, 35, 27, 29, 31))
obs$pop &lt;- obs$succ + obs$fail</code></pre>
<p>For data concatenated in this way (so that there isn’t an entry for every observation with a binary outcome, but instead a summary of the binomial outcome) we can fit logistic regression in a number of ways in R. One option is to give success probabilities and total population sizes with the <code>weights</code> option, and the other is to give a two-column dataframe of the success and failures. Both these options are shown here.</p>
<pre class="r"><code>fit_a &lt;- glm(succ / pop ~ x, data = obs, family = binomial, weights = obs$pop)
fit_b &lt;- glm(cbind(succ, fail) ~ x, data = obs, family = binomial)</code></pre>
<p>These function calls both produce the same estimates of the coefficients, deviance, etc.</p>
<pre class="r"><code>identical(coefficients(fit_a), coefficients(fit_b)) 
## [1] TRUE</code></pre>
</div>
<div id="in-julia" class="section level2">
<h2>In julia</h2>
<p>Julia is a relatively new high performance language that implements just-in-time compilation, so is really speedy for a lot of things that R just isn’t. To define dataframes like in R and fit glm’s two packages are needed.</p>
<pre class="julia"><code># packages are required for glm&#39;s and dataframes
Pkg.add(&quot;GLM&quot;)
Pkg.add(&quot;DataFrames&quot;)
using(GLM) 
using(DataFrames) 
</code></pre>
<p>To define the same data as used above in julia we use <code>DataFrame</code>. This is pretty similar to R, but notice that if you want to do vector operations you need the <code>./</code> syntax.</p>
<pre class="julia"><code>obs = DataFrame(x = [1.1, 3.3, 5.3, 2.1, 7.3, 3.6, 5.9, 4.6, 3.5, 4.3],
                succ = [23.0, 24, 17, 26, 15, 22, 20, 19, 19, 21],
                fail = [39.0, 35, 32, 28, 25, 31, 35, 27, 29, 31])
obs[:pop] = obs[:succ] .+ obs[:fail]
obs[:theta] = obs[:succ] ./ obs[:pop]
</code></pre>
<p>Fitting the regression is also very similar to option <code>a</code> in R (the two-column definition approach does not apply in julia), using <code>wts</code>.</p>
<pre class="julia"><code>fit = glm(@formula(theta ~ x), obs, Binomial(), LogitLink(), wts = obs[:pop])</code></pre>
</div>
<div id="sanity-check-between-r-and-julia" class="section level2">
<h2>Sanity check between R and julia</h2>
<p>Check that the two approaches produce the same estimates. In r:</p>
<pre class="r"><code>summary(fit_a)
## 
## Call:
## glm(formula = succ/pop ~ x, family = binomial, data = obs, weights = obs$pop)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.85032  -0.20715   0.07557   0.19383   0.99042  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -0.26432    0.22604  -1.169    0.242
## x           -0.03828    0.05281  -0.725    0.469
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2.7722  on 9  degrees of freedom
## Residual deviance: 2.2458  on 8  degrees of freedom
## AIC: 49.781
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>and in julia:</p>
<pre class="julia"><code>coef(fit)
## 2-element Array{Float64,1}:
##  -0.264318 
##  -0.0382826
deviance(fit)
## 2.2458348828330963
stderr(fit)
## 2-element Array{Float64,1}:
##  0.226044 
##  0.0528096</code></pre>
</div>
<div id="r-vs-julia" class="section level2">
<h2>R vs julia</h2>
<p>So what’s the point of using julia over R for fitting glm’s, especially when the documentation for this in julia is severely lacking in comparison? Well, julia is fast at doing a lot of things R isn’t, so if we needed to fit lots of regression models sequentially, we would see a significant time boost by using julia over R. In this example, if we just fit the same model 10000 times, julia does it about 10 times faster than R.</p>
<pre class="julia"><code>@elapsed for i in 1:10000
           glm(@formula(theta ~ x), obs, Binomial(), LogitLink(), wts = obs[:pop])
         end
## 1.200012779</code></pre>
<pre class="r"><code>system.time(
  for (i in 1:10000) {
    glm(succ / pop ~ x, data = obs, family = binomial, weights = obs$pop)
  }
)
##    user  system elapsed 
##   9.759   0.000   9.759</code></pre>
</div>
