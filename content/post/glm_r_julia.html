---
title: "Logistic regression: R vs Julia"
author: Alison Parton
output: html_document
date: 2018-02-22
tags: ["R","julia","glm"]
summary: "This is a quick comparison of simple logistic regression fitting in R and julia. Spoiler: julia is faster."
---



<p>This is a quick comparison of simple logistic regression fitting in R and julia. To implement this, you will need julia and the <code>JuliaCall</code> package in R. The following allows you to define a <code>julia chunk</code> in R markdown.</p>
<pre class="r"><code>knitr::knit_engines$set(julia = JuliaCall::eng_juliacall)</code></pre>
<div id="in-r" class="section level2">
<h2>In R</h2>
<p>Standard logistic regression is implemented easily in R with the <code>glm</code> function. In this example we’ll use the following data, that gives a continuous variable <code>x</code> and the numbers of successes, <code>succ</code>, and failures <code>fail</code>. The total population size for each observation of <code>x</code> is then successes plus failures.</p>
<pre class="r"><code>obs &lt;- data.frame(x = c(1.1, 3.3, 5.3, 2.1, 7.3, 3.6, 5.9, 4.6, 3.5, 4.3),
                  succ = c(23, 24, 17, 26, 15, 22, 20, 19, 19, 21),
                  fail = c(39, 35, 32, 28, 25, 31, 35, 27, 29, 31))
obs$pop &lt;- obs$succ + obs$fail</code></pre>
<p>For data concatenated in this way (so that there isn’t an entry for every observation with a binary outcome, but instead a summary of the binomial outcome) we can fit logistic regression in a number of ways in R. One option is to give success probabilities and total population sizes with the <code>weights</code> option, and the other is to give a two-column dataframe of the success and failures. Both these options are shown here.</p>
<pre class="r"><code>fit_a &lt;- glm(succ / pop ~ x, data = obs, family = binomial, weights = obs$pop)
fit_b &lt;- glm(cbind(succ, fail) ~ x, data = obs, family = binomial)</code></pre>
<p>These function calls both produce the same estimates of the coefficients, deviance, etc.</p>
<pre class="r"><code>identical(coefficients(fit_a), coefficients(fit_b)) 
## [1] TRUE</code></pre>
</div>
<div id="in-julia" class="section level2">
<h2>In julia</h2>
<p>Julia is a relatively new high performance language that implements just-in-time compilation, so is really speedy for a lot of things that R just isn’t. To define dataframes like in R and fit glm’s two packages are needed.</p>
<pre class="julia"><code># packages are required for glm&#39;s and dataframes
Pkg.add(&quot;GLM&quot;)
Pkg.add(&quot;DataFrames&quot;)
using(GLM) 
using(DataFrames) 
</code></pre>
<p>To define the same data as used above in julia we use <code>DataFrame</code>. This is pretty similar to R, but notice that if you want to do vector operations you need the <code>./</code> syntax.</p>
<pre class="julia"><code>obs = DataFrame(x = [1.1, 3.3, 5.3, 2.1, 7.3, 3.6, 5.9, 4.6, 3.5, 4.3],
                succ = [23.0, 24, 17, 26, 15, 22, 20, 19, 19, 21],
                fail = [39.0, 35, 32, 28, 25, 31, 35, 27, 29, 31])
obs[:pop] = obs[:succ] .+ obs[:fail]
obs[:theta] = obs[:succ] ./ obs[:pop]
</code></pre>
<p>Fitting the regression is also very similar to option <code>a</code> in R (the two-column definition approach does not apply in julia), using <code>wts</code>.</p>
<pre class="julia"><code>fit = glm(@formula(theta ~ x), obs, Binomial(), LogitLink(), wts = obs[:pop])</code></pre>
</div>
<div id="sanity-check-between-r-and-julia" class="section level2">
<h2>Sanity check between R and julia</h2>
<p>Check that the two approaches produce the same estimates. In r:</p>
<pre class="r"><code>summary(fit_a)
## 
## Call:
## glm(formula = succ/pop ~ x, family = binomial, data = obs, weights = obs$pop)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.85032  -0.20715   0.07557   0.19383   0.99042  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -0.26432    0.22604  -1.169    0.242
## x           -0.03828    0.05281  -0.725    0.469
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2.7722  on 9  degrees of freedom
## Residual deviance: 2.2458  on 8  degrees of freedom
## AIC: 49.781
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>and in julia:</p>
<pre class="julia"><code>fit
## StatsModels.DataFrameRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredChol{Float64,Base.LinAlg.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}
## 
## Formula: theta ~ 1 + x
## 
## Coefficients:
##                Estimate Std.Error   z value Pr(&gt;|z|)
## (Intercept)   -0.264318  0.226044  -1.16932   0.2423
## x            -0.0382826 0.0528096 -0.724917   0.4685

# examples of options for specific model values
coef(fit)
## 2-element Array{Float64,1}:
##  -0.264318 
##  -0.0382826
deviance(fit)
## 2.2458348828330963
stderr(fit)
## 2-element Array{Float64,1}:
##  0.226044 
##  0.0528096</code></pre>
</div>
<div id="r-vs-julia" class="section level2">
<h2>R vs julia</h2>
<p>So what’s the point of using julia over R for fitting glm’s, especially when the documentation for this in julia is severely lacking in comparison? Well, julia is fast at doing a lot of things R isn’t, so if we needed to fit lots of regression models sequentially, we would hope to see a significant time boost by using julia over R.</p>
<p>In this example, let’s assume we have 10000 sets of data coming in sequentially, and each time we want to fit the logistic regression and predict what the outcome would be for a new observation with <code>x=1</code>. Implement this in R as:</p>
<pre class="r"><code>r_lg_pred &lt;- function() {
  obs = data.frame(x = rnorm(10),
                   succ = sample(20:30, 10),
                   fail = sample(10:20, 10))
  obs$pop &lt;- obs$succ + obs$fail
  fit &lt;- glm(succ / pop ~ x, data = obs, family = binomial, weights = obs$pop)
  predict(fit, newdata = data.frame(x = 1.0))
}
r_store_pred &lt;- rep(NA, 10000)</code></pre>
<p>and in julia as:</p>
<pre class="julia"><code>function jul_lg_pred()
  obs = DataFrame(x = randn(10),
                  succ = rand(20.0:30, 10),
                  fail = rand(10.0:20, 10))
  obs[:pop] = obs[:succ] .+ obs[:fail]
  obs[:theta] = obs[:succ] ./ obs[:pop]
  fit = glm(@formula(theta ~ x), obs, Binomial(), LogitLink(), wts = obs[:pop])
  predict(fit, DataFrame(x = [1.0]))[1]
end
jul_store_pred = zeros(Float64, 10000)</code></pre>
<p>Timed runs show a potentially useful speed-up in julia</p>
<pre class="r"><code>system.time(
  for (i in 1:10000) {
    r_store_pred[i] &lt;- r_lg_pred()
  }
)
##    user  system elapsed 
##  17.369   0.004  17.373</code></pre>
<pre class="julia"><code>@elapsed for i in 1:10000
           jul_store_pred[i] = jul_lg_pred()
         end
## 2.829691777</code></pre>
</div>
